{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGmBLaBawMQ0ikupcFDavm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ğŸ”§ Fine-Tuning ResNet-50 on Real, Synthetic, and Mixed Fire Datasets\n","\n","This notebook fine-tunes ResNet-50 classification models using three dataset compositions:\n","- 100% real images from the D-Fire dataset\n","- 100% synthetic images from the Yunnan UE5-generated dataset\n","- 50/50 mixed composition using the `FireClassificationMixedDataset` class\n","\n","The goal is to assess whether fine-tuning improves model performance compared to feature extraction, especially when only limited real data is available. Fine-tuning is performed by unfreezing the final ResNet-50 block (`layer4`) and fully connected layer (`fc`), while keeping earlier layers frozen.\n","\n","Each model is trained for 5 epochs using the same training loop defined in `train_model.py`, with validation on the held-out D-Fire validation set. Results will be compared directly to their frozen counterparts from Phase 1.\n"],"metadata":{"id":"s1pQkO-uOy1J"}},{"cell_type":"markdown","source":["## ğŸ“¦ Notebook Setup: Mount Drive & Clone GitHub Repo\n","\n","This cell ensures the notebook is reproducible in any new Colab session by:\n","\n","- Mounting your Google Drive (to access datasets, secrets, and checkpoints)\n","- Loading your GitHub token from Drive\n","- Cloning the fire-detection-dissertation repository\n","- Navigating into the correct folder\n","- Setting Git identity for future commits\n","\n","âš ï¸ **Note:** This cell must be run every time you open this notebook in a new Colab session.\n"],"metadata":{"id":"S9eIgnXxOye4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8Wb3b_UN773"},"outputs":[],"source":["# ğŸ”§ Minimal Colab setup for any working notebook\n","\n","# 1. Mount Google Drive\n","import os\n","from google.colab import drive\n","if not os.path.ismount(\"/content/drive\"):\n","    drive.mount(\"/content/drive\")\n","\n","# 2. Load GitHub token securely from Drive\n","token_path = \"/content/drive/MyDrive/fire-detection-dissertation/secrets/github_token.txt\"\n","with open(token_path, \"r\") as f:\n","    token = f.read().strip()\n","\n","# 3. Clone the GitHub repo (force fresh clone for safety)\n","username = \"Misharasapu\"\n","repo = \"fire-detection-dissertation\"\n","clone_url = f\"https://{token}@github.com/{username}/{repo}.git\"\n","repo_path = f\"/content/{repo}\"\n","\n","# Optional: Remove old clone (safe to rerun)\n","!rm -rf {repo_path}\n","\n","# Clone fresh and move into the repo\n","%cd /content\n","!git clone {clone_url}\n","%cd {repo}\n","\n","# 4. Set Git identity (required in Colab sessions)\n","!git config --global user.name \"Misharasapu\"\n","!git config --global user.email \"misharasapu@gmail.com\"\n"]},{"cell_type":"markdown","source":["## ğŸ§© Step 1: Define Dataset Type, Paths, and Model Filenames\n","\n","In this step, we define the training configuration for each model to be fine-tuned. Three dataset types are used:\n","\n","- ğŸ”µ `real`: 100% real images (D-Fire)\n","- ğŸŸ  `synthetic`: 100% synthetic images (Yunnan UE5)\n","- ğŸŸ£ `mixed`: 50% synthetic, 50% real\n","\n","We also define model filenames dynamically and ensure reproducibility with a fixed random seed. The total number of training samples is kept constant for the mixed dataset (5,260), and filenames are structured to reflect the data composition used during training.\n"],"metadata":{"id":"Ylo-h99xWD05"}},{"cell_type":"code","source":["import os\n","import random\n","import torch\n","import numpy as np\n","\n","# ğŸ”§ Adjustable configuration\n","syn_ratio = 0.50                    # Used only for the mixed dataset\n","total_samples = 5260                # Fixed total sample size for mixed dataset\n","\n","# ğŸ—‚ï¸ Paths to training data\n","real_image_dir = \"/content/drive/MyDrive/fire-detection-dissertation/data/raw/real/D-Fire/train/images\"\n","real_label_dir = \"/content/drive/MyDrive/fire-detection-dissertation/data/raw/real/D-Fire/train/labels\"\n","\n","# Yunnan synthetic (confirmed 'synthetic_all' exists)\n","syn_image_dir  = \"/content/drive/MyDrive/fire-detection-dissertation/data/raw/synthetic/yunnan/synthetic_all/images\"\n","syn_label_dir  = \"/content/drive/MyDrive/fire-detection-dissertation/data/raw/synthetic/yunnan/synthetic_all/labels\"\n","\n","# ğŸ§  Model filenames based on dataset type (match existing Phase 2 naming)\n","model_paths = {\n","    \"real\":       \"resnet_outdoor_real_100_ft_phase2.pt\",\n","    \"synthetic\":  \"resnet_outdoor_synthetic_100_ft_phase2.pt\",\n","    \"mixed\":      f\"resnet_outdoor_{int(syn_ratio*100)}syn_{int((1-syn_ratio)*100)}real_ft_phase2.pt\"\n","}\n","\n","# âœ… Set random seed for reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n"],"metadata":{"id":"DN_Bsc3vPRzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ğŸ§¼ Step 2: Define Image Transformations\n","\n","We define the image transformation pipeline used across all datasets. Images are resized to 224Ã—224, normalized to ImageNet statistics, and converted to PyTorch tensors.\n","\n","This ensures compatibility with the input format expected by ResNet-50, which was pretrained on ImageNet.\n"],"metadata":{"id":"BU-4MJwtcSa9"}},{"cell_type":"code","source":["# --- Step 2: Image transforms (train + val only, no augmentation, no Normalize) ---\n","\n","from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),            # ğŸ‘ˆ no Normalize\n","])\n","\n","print(\"Transforms set: Resize(224,224) â†’ ToTensor()  [no normalization]\")"],"metadata":{"id":"fXX6CpoAZLAl","executionInfo":{"status":"ok","timestamp":1755626168704,"user_tz":-60,"elapsed":7106,"user":{"displayName":"Mishara Sapukotanage","userId":"02206396968881771481"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c747a91-002e-4813-ab77-b8299d1e1fef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transforms set: Resize(224,224) â†’ ToTensor()  [no normalization]\n"]}]},{"cell_type":"markdown","source":["## âš™ï¸ Step 3: Load Dataset, Pretrained Model, and Configure Fine-Tuning\n","\n","Based on the selected training mode (`\"real\"`, `\"synthetic\"`, or `\"mixed\"`), we perform the following:\n","\n","1. Load the appropriate dataset class with the predefined transform:\n","   - `FireClassificationDataset` for real images (D-Fire)\n","   - `FireClassificationSyntheticDataset` for synthetic images (Yunnan UE5)\n","   - `FireClassificationMixedDataset` for a fixed-ratio combination of both\n","\n","2. Split the full dataset into training and validation subsets using an 80/20 split with a fixed random seed (`seed=42`) to ensure reproducibility.\n","\n","3. Load the corresponding Phase 1 model (`*_100.pt`), which was trained using feature extraction (frozen base, trained `fc` layer only).\n","\n","4. Construct the Adam optimizer using only the parameters marked with `requires_grad=True`. Fine-tuning will be handled automatically inside the `train_model()` helper by unfreezing `layer4` and `fc` when `fine_tune=True`.\n","\n","This setup ensures consistent dataset usage and training conditions across all modes, and enables us to isolate the impact of fine-tuning deeper convolutional layers on classification performance.\n"],"metadata":{"id":"DN8PurielUdY"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, random_split\n","from torchvision import models\n","from torch import nn, optim\n","import os\n","\n","from utils.fire_classification_dataset import (\n","    FireClassificationDataset,\n","    FireClassificationMixedDataset\n",")\n","\n","# ğŸ”˜ SELECT WHICH MODEL TO TRAIN\n","selected_mode = \"real\"  # Options: \"real\", \"synthetic\", \"mixed\"\n","print(f\"\\nğŸ“¦ Selected mode: {selected_mode.upper()}\")\n","\n","# ğŸ–¥ï¸ Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","using_gpu = torch.cuda.is_available()\n","print(f\"ğŸ–¥ï¸ Using device: {device} ({'GPU enabled' if using_gpu else 'CPU only'})\")\n","\n","# âœ… Training parameters\n","batch_size = 32\n","num_epochs = 5\n","learning_rate = 1e-4\n","print(f\"ğŸ“Œ Training config â†’ Batch size: {batch_size}, Epochs: {num_epochs}, LR: {learning_rate}\")\n","\n","# âœ… Build dataset by mode\n","if selected_mode == \"real\":\n","    print(\"ğŸ”¹ Loading 100% real dataset (D-Fire)...\")\n","    full_dataset = FireClassificationDataset(\n","        image_dir=real_image_dir,\n","        label_dir=real_label_dir,\n","        transform=transform,\n","        dataset_type=\"real\"\n","    )\n","    pretrained_path = \"/content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_phase1.pt\"\n","\n","elif selected_mode == \"synthetic\":\n","    print(\"ğŸ”¸ Loading 100% synthetic dataset (Yunnan)...\")\n","    full_dataset = FireClassificationDataset(\n","        image_dir=syn_image_dir,\n","        label_dir=syn_label_dir,\n","        transform=transform,\n","        dataset_type=\"synthetic\"\n","    )\n","    pretrained_path = \"/content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_synthetic_100_phase1.pt\"\n","\n","elif selected_mode == \"mixed\":\n","    print(\"ğŸŸ£ Loading mixed dataset (Real + Synthetic)...\")\n","    print(f\"   â†’ Synthetic ratio: {syn_ratio:.2f}, Total samples: {total_samples}\")\n","    full_dataset = FireClassificationMixedDataset(\n","        real_image_dir=real_image_dir,\n","        real_label_dir=real_label_dir,\n","        syn_image_dir=syn_image_dir,\n","        syn_label_dir=syn_label_dir,\n","        syn_ratio=syn_ratio,\n","        total_samples=total_samples,\n","        transform=transform\n","    )\n","    pretrained_path = (\n","        f\"/content/drive/MyDrive/fire-detection-dissertation/models/\"\n","        f\"resnet_outdoor_{int(syn_ratio*100)}syn_{int((1-syn_ratio)*100)}real_phase1.pt\"\n","    )\n","else:\n","    raise ValueError(\"âŒ Invalid mode. Choose from: 'real', 'synthetic', or 'mixed'.\")\n","\n","# âœ… Split into train / val\n","train_ratio = 0.8\n","train_size = int(train_ratio * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","generator = torch.Generator().manual_seed(42)\n","\n","train_data, val_data = random_split(full_dataset, [train_size, val_size], generator=generator)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","print(f\"ğŸ“Š Dataset split â†’ Train: {train_size}, Validation: {val_size}\")\n","\n","# âœ… Load Phaseâ€‘1 checkpoint safely\n","print(f\"\\nğŸ”§ Loading pretrained Phase 1 model: {pretrained_path}\")\n","if not os.path.exists(pretrained_path):\n","    raise FileNotFoundError(f\"Checkpoint not found at: {pretrained_path}\")\n","\n","# Note: torchvision warns about 'pretrained=True' in newer versions; this keeps parity with your earlier runs.\n","model = models.resnet50(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 2)\n","\n","state = torch.load(pretrained_path, map_location=device)\n","model.load_state_dict(state)\n","model = model.to(device)\n","print(\"âœ… Model loaded and moved to device.\")\n","\n","# ğŸ”“ FINEâ€‘TUNE POLICY (Phase 2): unfreeze only layer4 + fc\n","print(\"\\nğŸ§© Fineâ€‘tuning setup â†’ Freeze all, then unfreeze layer4 + fc\")\n","for p in model.parameters():\n","    p.requires_grad = False\n","for p in model.layer4.parameters():\n","    p.requires_grad = True\n","for p in model.fc.parameters():\n","    p.requires_grad = True\n","\n","# Show a quick summary of what's trainable\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"ğŸ” Trainable params: {trainable_params:,} / {total_params:,} \"\n","      f\"({100.0 * trainable_params / total_params:.2f}% of model)\")\n","\n","# âœ… Optimizer only on trainable params\n","optimizer = optim.Adam((p for p in model.parameters() if p.requires_grad), lr=learning_rate)\n","print(\"ğŸ› ï¸ Optimizer configured (Adam) on layer4 + fc only â€” fineâ€‘tuning mode active.\")\n","\n","# âœ… Save path for the fineâ€‘tuned model\n","save_path = f\"/content/drive/MyDrive/fire-detection-dissertation/models/{model_paths[selected_mode]}\"\n","print(f\"ğŸ’¾ Fineâ€‘tuned model will be saved to: {save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pceHNfeakfCl","executionInfo":{"status":"ok","timestamp":1755626339828,"user_tz":-60,"elapsed":987,"user":{"displayName":"Mishara Sapukotanage","userId":"02206396968881771481"}},"outputId":"7333f2a4-4e14-4bb0-82fc-6580f4f670ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“¦ Selected mode: REAL\n","ğŸ–¥ï¸ Using device: cpu (CPU only)\n","ğŸ“Œ Training config â†’ Batch size: 32, Epochs: 5, LR: 0.0001\n","ğŸ”¹ Loading 100% real dataset (D-Fire)...\n","ğŸ“Š Dataset split â†’ Train: 13777, Validation: 3445\n","\n","ğŸ”§ Loading pretrained Phase 1 model: /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_phase1.pt\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Model loaded and moved to device.\n","\n","ğŸ§© Fineâ€‘tuning setup â†’ Freeze all, then unfreeze layer4 + fc\n","ğŸ” Trainable params: 14,968,834 / 23,512,130 (63.66% of model)\n","ğŸ› ï¸ Optimizer configured (Adam) on layer4 + fc only â€” fineâ€‘tuning mode active.\n","ğŸ’¾ Fineâ€‘tuned model will be saved to: /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_ft_phase2.pt\n"]}]},{"cell_type":"markdown","source":["## ğŸ” Step 4: Train the Fine-Tuned Model (layer4 + fc)\n","\n","In this step, we begin training the selected model using the `train_model()` helper function.\n","\n","- `fine_tune=True` ensures that both `layer4` and `fc` are unfrozen and trained.\n","- The model weights are initialized from the Phase 1 checkpoint (trained with feature extraction).\n","- The model will be trained for 5 additional epochs using the same dataset composition as in Phase 1.\n","- Results will be printed at the end of each epoch and the best-performing model (by F1 score) will be saved to the specified path.\n"],"metadata":{"id":"TLVcdlNOq8SY"}},{"cell_type":"code","source":["from utils.train_model import train_model\n","from torch.nn import CrossEntropyLoss\n","\n","# âœ… Define loss function\n","criterion = CrossEntropyLoss()\n","\n","# âœ… Start training (with fine-tuning)\n","train_losses, val_losses = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    num_epochs=num_epochs,\n","    device=device,\n","    save_path=save_path,\n","    print_every=1,\n","    print_batch_loss=False\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBIaToEXoNgV","outputId":"0854b111-b31c-43d1-d338-25ebde9ae4a5","executionInfo":{"status":"ok","timestamp":1755661547168,"user_tz":-60,"elapsed":6032473,"user":{"displayName":"Mishara Sapukotanage","userId":"02206396968881771481"}}},"execution_count":7,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","ğŸ” Model device: cpu\n","ğŸ“Š Trainable parameters: 14968834\n","\n","ğŸ” Epoch 1/5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ… Epoch [1/5] | Train Loss: 0.1366 | Val Loss: 0.0937 | Acc: 0.9689 | Precision: 0.9497 | Recall: 0.9343 | F1: 0.9419 | Time: 16591.3s\n","ğŸ’¾ New best model saved (F1: 0.9419) â†’ /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_ft_phase2.pt\n","\n","ğŸ” Epoch 2/5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ… Epoch [2/5] | Train Loss: 0.0499 | Val Loss: 0.1230 | Acc: 0.9608 | Precision: 0.9553 | Recall: 0.8967 | F1: 0.9250 | Time: 4633.5s\n","\n","ğŸ” Epoch 3/5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ… Epoch [3/5] | Train Loss: 0.0321 | Val Loss: 0.1272 | Acc: 0.9704 | Precision: 0.9366 | Recall: 0.9548 | F1: 0.9456 | Time: 4691.5s\n","ğŸ’¾ New best model saved (F1: 0.9456) â†’ /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_ft_phase2.pt\n","\n","ğŸ” Epoch 4/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["âœ… Epoch [4/5] | Train Loss: 0.0221 | Val Loss: 0.1066 | Acc: 0.9724 | Precision: 0.9484 | Recall: 0.9494 | F1: 0.9489 | Time: 4680.4s\n","ğŸ’¾ New best model saved (F1: 0.9489) â†’ /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_ft_phase2.pt\n","\n","ğŸ” Epoch 5/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["âœ… Epoch [5/5] | Train Loss: 0.0145 | Val Loss: 0.1251 | Acc: 0.9733 | Precision: 0.9614 | Recall: 0.9386 | F1: 0.9499 | Time: 4607.4s\n","ğŸ’¾ New best model saved (F1: 0.9499) â†’ /content/drive/MyDrive/fire-detection-dissertation/models/resnet_outdoor_real_100_ft_phase2.pt\n"]}]},{"cell_type":"code","source":["# âœ… 1. Navigate to the Git-tracked repo folder\n","%cd /content/fire-detection-dissertation\n","\n","# âœ… 2. Move the notebook from Drive into the repo so Git can track it\n","!cp /content/drive/MyDrive/fire-detection-dissertation/notebooks/06_train_resnet_finetuned.ipynb /content/fire-detection-dissertation/notebooks/\n","\n","# Optional: confirm it's now inside the repo\n","!ls notebooks/\n","\n","# âœ… 3. Stage the notebook for commit\n","!git add notebooks/06_train_resnet_finetuned.ipynb\n","\n","# âœ… 4. Commit with a message\n","!git commit -m \"Refactored Phase 2 fine-tuning notebook with unnormalised pipeline and updated dataset handling\"\n","\n","# âœ… 5. Push to GitHub\n","!git push\n"],"metadata":{"id":"1JQID-HWZhTu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755684306783,"user_tz":-60,"elapsed":542,"user":{"displayName":"Mishara Sapukotanage","userId":"02206396968881771481"}},"outputId":"ccfce2b2-3a66-4e70-c8b2-b60187f8a9f0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/fire-detection-dissertation'\n","/content\n","cp: cannot stat '/content/drive/MyDrive/fire-detection-dissertation/notebooks/06_train_resnet_finetuned.ipynb': No such file or directory\n","ls: cannot access 'notebooks/': No such file or directory\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0B9JW5Jk8Hvy"},"execution_count":null,"outputs":[]}]}